{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "608c2e2c",
   "metadata": {},
   "source": [
    "# Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f65f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from torchtyping import TensorType as TT\n",
    "from typing import List, Union, Optional\n",
    "from jaxtyping import Float, Int\n",
    "from functools import partial\n",
    "import itertools\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizer\n",
    "import datasets\n",
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import HTML\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370fd3d9",
   "metadata": {},
   "source": [
    "Load your model here. Input the model path.\n",
    "\n",
    "Remember to adjust parameters in Transformer_lens.HookedTransformer.from_pretrained()\n",
    "\n",
    "Some models may cause errors from Transformer_lens.HookedTransformer\n",
    "\n",
    "Make sure that your model is supported by Transformer_lens or put in right parameters in HookedTransformer.from_pretrained() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1cc0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH='path/to/your/model/'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "hf_model = LlamaForCausalLM.from_pretrained(MODEL_PATH)\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", \n",
    "                                          hf_model=hf_model, \n",
    "                                          device='cuda',\n",
    "                                          fold_ln=False, \n",
    "                                          center_writing_weights=False, \n",
    "                                          center_unembed=False, \n",
    "                                          tokenizer=tokenizer\n",
    "                                         )\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e48520",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T07:17:32.489085Z",
     "start_time": "2024-06-08T07:17:32.479194Z"
    }
   },
   "source": [
    "Necessary algorithm\n",
    "\n",
    "Adjust judge function according to you model to test here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a8dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_tensor(dataMat, topNfeat=9999999):\n",
    "    dataMat_np = dataMat.numpy()\n",
    "    n_components=topNfeat if topNfeat is not None else dataMat.shape[1]\n",
    "    pca = PCA(n_components = n_components)\n",
    "    lowDDataMat_np = pca.fit_transform(dataMat_np)\n",
    "    lowDDataMat = torch.tensor(lowDDataMat_np, dtype=dataMat.dtype)\n",
    "    return lowDDataMat\n",
    "\n",
    "def judge(token_id):\n",
    "    token = tokenizer.decode([token_id])\n",
    "    token_id = torch.tensor([token_id])\n",
    "    text1 = \"Question: Can you repeat the string '\"\n",
    "    text2 = \"and return back to me?\\n\\nAnswer: Here is the repeated string:\\n\\n\"\n",
    "    tokenseq1 = model.to_tokens(text1)[0]\n",
    "    tokenseq2 = model.to_tokens(text2)[0]\n",
    "    tokens = torch.cat((tokenseq1, token_id.to(device1), torch.tensor([29915]).to(device1), tokenseq2[1:]), dim=0)\n",
    "    tokens = torch.unsqueeze(tokens, dim=0)\n",
    "    text = tokenizer.decode(tokens.tolist()[0])\n",
    "    k = len(text)\n",
    "    response = model.generate(tokens, max_new_tokens=10, temperature=0, verbose=False, return_type='str')[k:]\n",
    "    if token in response or token.upper() in response.upper():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea876aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52a0cbb3",
   "metadata": {},
   "source": [
    "# Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb98335",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_data = torch.load('caches/Llama-2-7b-chat-pca-data.pt')\n",
    "token_ids = torch.arange(1, 32000)\n",
    "split_share = 0.1\n",
    "for i in range(train_datasize):\n",
    "    token_id = token_ids[i]\n",
    "    if token_id in all_glitch_tokens:\n",
    "        glitch_tokens.append(token_id)\n",
    "        glitch_caches.append(token_data[i])\n",
    "    else:\n",
    "        normal_tokens.append(token_id)\n",
    "        normal_caches.append(token_data[i])\n",
    "\n",
    "indices = torch.randperm(token_data.size(0))\n",
    "shuffled_token_ids = token_ids[indices]\n",
    "shuffled_token_data = token_data[indices]\n",
    "train_labels = []\n",
    "train_size = int(token_data.size(0) * split_share)\n",
    "test_size = token_data.size(0) - train_size\n",
    "train_tokens, test_tokens = shuffled_token_ids.split([train_size, test_size])\n",
    "train_data, test_data = shuffled_token_data.split([train_size, test_size])\n",
    "for t in train_tokens:\n",
    "    if judge(t):\n",
    "        train_labels.append(0)\n",
    "    else:\n",
    "        train_labels.append(1)\n",
    "test_labels = np.array([1 if token in all_glitch_token else 0 for token in test_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd19d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = [(1,3)]\n",
    "for (C_val, degree_val) in parameter:\n",
    "    svm_model = SVC(C=C_val, kernel='poly', degree=degree_val, class_weight='balanced', probability=True)\n",
    "    svm_model.fit(train_data, train_labels)\n",
    "    predictions = svm_model.predict(test_data)\n",
    "    for idx, pred in enumerate(predictions):\n",
    "        if pred == 1:\n",
    "            if judge(test_tokens[idx]):\n",
    "                predictions[idx] = 0\n",
    "precision = precision_score(true_labels, predictions, pos_label=1)\n",
    "recall = recall_score(true_labels, predictions, pos_label=1)\n",
    "f1 = f1_score(true_labels, predictions, pos_label=1)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a246cd",
   "metadata": {},
   "source": [
    "# Fix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca89aa7",
   "metadata": {},
   "source": [
    "Load files of the Neun\\up and Neun\\down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11008247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlp_pre_layer_dim = torch.load('caches/llama_fix_mlp_pre_layer_dim.pt')\n",
    "mlp_pre_linear_layer_dim = torch.load('caches/llama_fix_mlp_pre_linear_layer_dim.pt')\n",
    "mlp_pre_layer_dim_non = torch.load('caches/llama_fix_mlp_pre_layer_dim_non.pt')\n",
    "mlp_pre_linear_layer_dim_non = torch.load('caches/llama_fix_mlp_pre_linear_layer_dim_non.pt')\n",
    "\n",
    "mlp_pre_indices = torch.load('caches/llama_fix_mlp_pre_indices.pt')\n",
    "mlp_pre_linear_indices = torch.load('caches/llama_fix_mlp_pre_linear_indices.pt')\n",
    "mlp_pre_indices_non = torch.load('caches/llama_fix_mlp_pre_indices_non.pt')\n",
    "mlp_pre_linear_indices_non = torch.load('caches/llama_fix_mlp_pre_linear_indices_non.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52d6b2a",
   "metadata": {},
   "source": [
    "Adjust the k and b param according to your model, to reach the best performance in the fix algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_1 = 25\n",
    "b_1 = 0.5\n",
    "k_2 = 2\n",
    "b_2 = 0\n",
    "def compute_activation_difference(normal_activations, glitch_activations):\n",
    "    differences = []\n",
    "    layer = layer_start\n",
    "    for normal_layer, glitch_layer in zip(normal_activations, glitch_activations):\n",
    "        normal_mean = np.mean(normal_layer, axis=0)\n",
    "        # print(f\"layer = {layer} normal_mean = {normal_mean}\")\n",
    "        glitch_mean = np.mean(glitch_layer, axis=0)\n",
    "        # print(f\"layer = {layer} glitch_mean = {glitch_mean}\")\n",
    "        differences.append(np.mean(normal_mean - glitch_mean))\n",
    "        layer+=1\n",
    "    return np.mean(differences)\n",
    "\n",
    "def compute_activation_non_difference(normal_activations, glitch_activations):\n",
    "    differences = []\n",
    "    for normal_layer, glitch_layer in zip(normal_activations, glitch_activations):\n",
    "        normal_mean = np.mean(normal_layer, axis=0)\n",
    "        glitch_mean = np.mean(glitch_layer, axis=0)\n",
    "        differences.append(np.mean(glitch_mean / normal_mean))\n",
    "    return np.mean(differences)\n",
    "\n",
    "def determine_alpha(difference):\n",
    "    return min(5, max(2, b_2 + difference * k_2))\n",
    "def determine_beta(difference):\n",
    "    return max(0, min(3, b_1 + difference * k_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b745ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T07:21:23.205630Z",
     "start_time": "2024-06-08T07:21:23.201566Z"
    }
   },
   "source": [
    "Load files about the difference\n",
    "\n",
    "Calculate needed parameter ALPHA and BETA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddf213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_difference_mlp_pre = torch.load('caches/llama_fix_average_difference_mlp_pre.pt')\n",
    "average_difference_mlp_pre_linear = torch.load('caches/llama_fix_average_difference_mlp_pre_linear.pt')\n",
    "average_difference_mlp_pre_non = torch.load('caches/llama_fix_average_difference_mlp_pre_non.pt')\n",
    "average_difference_mlp_pre_linear_non = torch.load('caches/llama_fix_average_difference_mlp_pre_linear_non.pt')\n",
    "\n",
    "alpha1 = determine_alpha(average_difference_mlp_pre_non)\n",
    "# print(\"Computed alpha1:\", alpha1)\n",
    "alpha2 = determine_alpha(average_difference_mlp_pre_linear_non)\n",
    "# print(\"Computed alpha2:\", alpha2)\n",
    "beta1 = determine_beta(average_difference_mlp_pre)\n",
    "# print(\"Computed beta1:\", beta1)\n",
    "beta2 = determine_beta(average_difference_mlp_pre_linear)\n",
    "# print(\"Computed beta2:\", beta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3837c812",
   "metadata": {},
   "source": [
    "Fix function to adjust keylayers in your model\n",
    "\n",
    "Adjust the k and b param according to your model, to reach the best performance in the fix algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d513c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fix(tokens, times, max_new_tokens=10):\n",
    "    key_layers = np.arange(19, 29)\n",
    "    \n",
    "    def mlp_pre_hook_layer(\n",
    "        value: Float[torch.Tensor, \"batch pos d_mlp\"],\n",
    "        hook: HookPoint,\n",
    "        layer: int\n",
    "    ) -> Float[torch.Tensor, \"batch pos d_mlp\"]:\n",
    "        #print(f\"Shape of the value tensor: {value.shape}\")\n",
    "        array_ = np.array(value[0][-1].cpu())\n",
    "        glitch_dim = []\n",
    "        while True:\n",
    "            if array_[array_.argmax()] > 1:\n",
    "                glitch_dim.append(array_.argmax())\n",
    "                array_[array_.argmax()] = -1000\n",
    "            else:\n",
    "                break\n",
    "        if layer <= max(mlp_pre_layer_dim_non[0]):\n",
    "            current_layer_indices1 = mlp_pre_layer_dim_non[1][mlp_pre_indices_non[layer]:mlp_pre_indices_non[layer+1]]\n",
    "            glitch_more = list(set(glitch_dim).intersection(current_layer_indices1))\n",
    "            # / alpha 1\n",
    "            value[:, -1, glitch_more]  /= alpha1\n",
    "        \n",
    "        if layer <= max(mlp_pre_layer_dim[0]):\n",
    "            current_layer_indices2 = mlp_pre_layer_dim[1][mlp_pre_indices[layer]:mlp_pre_indices[layer+1]]\n",
    "            normal_more = list(set(current_layer_indices2).difference(glitch_dim))\n",
    "            # + beta 1\n",
    "            value[:, -1, mlp_pre_layer_dim[1][mlp_pre_indices[layer]:mlp_pre_indices[layer+1]]]  = torch.abs(value[:, -1, mlp_pre_layer_dim[1][mlp_pre_indices[layer]:mlp_pre_indices[layer+1]]]) + beta1       \n",
    "\n",
    "        return value\n",
    "    \n",
    "    def mlp_pre_linear_hook_layer(\n",
    "        value: Float[torch.Tensor, \"batch pos d_mlp\"],\n",
    "        hook: HookPoint,\n",
    "        layer: int\n",
    "    ) -> Float[torch.Tensor, \"batch pos d_mlp\"]:\n",
    "        #print(f\"Shape of the value tensor: {value.shape}\")\n",
    "        array_ = np.array(value[0][-1].cpu())\n",
    "        glitch_dim = []\n",
    "        while True:\n",
    "            if array_[array_.argmax()] > 1:\n",
    "                glitch_dim.append(array_.argmax())\n",
    "                array_[array_.argmax()] = -1000\n",
    "            else:\n",
    "                break\n",
    "        #print(layer)\n",
    "        \n",
    "        if layer <= max(mlp_pre_linear_layer_dim_non[0]):  \n",
    "            current_layer_linear_indices1 = mlp_pre_linear_layer_dim_non[1][mlp_pre_linear_indices_non[layer]:mlp_pre_linear_indices_non[layer+1]]\n",
    "            glitch_more = list(set(glitch_dim).intersection(current_layer_linear_indices1))\n",
    "            # / alpha2\n",
    "            value[:, -1, glitch_more] /= alpha2 \n",
    "        if layer <= max(mlp_pre_linear_layer_dim[0]): \n",
    "            current_layer_indices2 = mlp_pre_linear_layer_dim[1][mlp_pre_linear_indices[layer]:mlp_pre_linear_indices[layer+1]]\n",
    "            normal_more = list(set(current_layer_indices2).difference(glitch_dim))\n",
    "            # + beta2\n",
    "            value[:, -1, mlp_pre_linear_layer_dim[1][mlp_pre_linear_indices[layer]:mlp_pre_linear_indices[layer+1]]] = torch.abs(value[:, -1, mlp_pre_linear_layer_dim[1][mlp_pre_linear_indices[layer]:mlp_pre_linear_indices[layer+1]]]) + beta2\n",
    "  \n",
    "        \n",
    "        return value\n",
    "    \n",
    "    \n",
    "    logits, cache = model.run_with_cache(tokens)\n",
    "    response_tokens = []\n",
    "    fwd_hooks = []\n",
    "    func_list = []\n",
    "    for layer in key_layers:\n",
    "        temp_hook_fn = partial(mlp_pre_hook_layer, layer=layer)\n",
    "        fwd_hooks.append((f'blocks.{layer}.mlp.hook_pre', temp_hook_fn))\n",
    "        temp_hook_fn_ = partial(mlp_pre_linear_hook_layer, layer=layer)\n",
    "        fwd_hooks.append((f'blocks.{layer}.mlp.hook_pre_linear', temp_hook_fn_))\n",
    "    \n",
    "#    hook model to adjust logits\n",
    "    for i in range(max_new_tokens):\n",
    "        aug_logits = model.run_with_hooks(tokens, \n",
    "                         return_type='logits',\n",
    "                         fwd_hooks=fwd_hooks)[0][-1]\n",
    "        \n",
    "        tokens = torch.cat((tokens[0], torch.tensor([aug_logits.argmax()]).to(device)),dim=0)\n",
    "        tokens = torch.unsqueeze(tokens, dim=0)\n",
    "#         generate new tokens\n",
    "        response_tokens.append(aug_logits.argmax())\n",
    "        \n",
    "        if aug_logits.argmax() == 2:\n",
    "            break\n",
    "    return model.to_string(torch.tensor(response_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac76821c",
   "metadata": {},
   "source": [
    "Record the remaining glitch tokens in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0058af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_7b = {'index':[], 'token':[]}\n",
    "path = 'caches/Llama-2-7b-chat-glitch-fix.csv'\n",
    "for token_id in all_glitch_tokens:\n",
    "    token = tokenizer.decode([token_id])\n",
    "    token_id = torch.tensor([token_id])\n",
    "    text1 = \"Question: Can you repeat the string '\"\n",
    "    text2 = \"and return back to me?\\n\\nAnswer: Here is the repeated string:\\n\\n\"\n",
    "    tokens1 = model.to_tokens(text1)[0]\n",
    "    tokens2 = model.to_tokens(text2)[0]\n",
    "    tokens = torch.cat((tokens1, token_id.to(device), torch.tensor([29915]).to(device), tokens2[1:]), dim=0)\n",
    "    tokens = torch.unsqueeze(tokens, dim=0)\n",
    "    response = generate_2(tokens, times = 5, max_new_tokens=10)\n",
    "    \n",
    "    if token in response or token.upper() in response.upper():\n",
    "        None\n",
    "    else:\n",
    "        chat_7b['index'].append(token_id)\n",
    "        chat_7b['token'].append(token)\n",
    "    if token_id%1000 == 0:\n",
    "        print(token_id, len(chat_7b['index']))\n",
    "        df_ = pd.DataFrame(chat_7b)\n",
    "        df_.to_csv(path, escapechar=',')\n",
    "df_ = pd.DataFrame(chat_7b)\n",
    "df_.to_csv(path, escapechar=',')\n",
    "repaired = len(all_glitch_tokens) - len(df_['index'].tolist())\n",
    "print(f\"Repaired tokens = {repaired}\")\n",
    "print(f\"Repaired rate = {repaired / len(all_glitch_tokens)}.4f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a35dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
